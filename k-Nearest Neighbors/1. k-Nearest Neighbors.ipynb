{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **k-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a table summarizing the k-Nearest Neighbors algorithm:\n",
    "\n",
    "| Step | Description |\n",
    "|---|---|\n",
    "| 1. **Identify Neighbors** |  Locate the *k* closest labeled data points to the unlabeled data point.  The value of *k* is a parameter set by the user. |\n",
    "| 2. **Majority Vote** | Determine the most frequent label among the *k* nearest neighbors. |\n",
    "| 3. **Prediction** | Assign the most frequent label (from step 2) as the predicted label for the unlabeled data point. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions: Using Scikit-Learn to Fit a KNN Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-by-Step Guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Import the Necessary Library:**\n",
    "\n",
    "    - Begin by importing the KNeighborsClassifier class from the sklearn.neighbors module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Prepare Your Data:**\n",
    "\n",
    "    - Split your dataset into X (features) and y (target values).\n",
    "    - Ensure X is a 2D array where each column represents a feature and each row represents an observation.\n",
    "    - Ensure y is a 1D array with the same number of observations as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "X = dataset[['feature1', 'feature2']].values  # Features\n",
    "y = dataset['churn_status'].values           # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Convert Data to NumPy Arrays:**\n",
    "\n",
    "    - Use the .values attribute to convert X and y to NumPy arrays if they are not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values  # Convert to NumPy array if not already\n",
    "y = y.values  # Convert to NumPy array if not already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Check the Shape of the Data:**\n",
    "\n",
    "    - Print the shape of X and y to verify the number of observations and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)  # Should output (3333, 2) if there are 3333 observations and 2 features\n",
    "print(y.shape)  # Should output (3333,) if there are 3333 target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Instantiate the KNN Classifier:**\n",
    "\n",
    "    - Create an instance of KNeighborsClassifier, specifying the number of neighbors with the n_neighbors parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Fit the Classifier to the Data:**\n",
    "\n",
    "    - Use the .fit method of the classifier to train it on the labeled data. Pass X (features) and y (target values) as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Code**\n",
    "\n",
    "Here's the complete code to fit a KNN classifier using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = dataset[['feature1', 'feature2']].values  # Features\n",
    "y = dataset['churn_status'].values           # Target\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X.shape)  # Output: (3333, 2)\n",
    "print(y.shape)  # Output: (3333,)\n",
    "\n",
    "# Instantiate the KNN classifier with 15 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using scikit-learn to fit a classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assuming 'churn_df' is a pandas DataFrame with columns 'total_day_charge', 'total_eve_charge', and 'churn'\n",
    "X = churn_df[[\"total_day_charge\", \"total_eve_charge\"]].values  # Features\n",
    "y = churn_df[\"churn\"].values  # Target variable\n",
    "\n",
    "print(X.shape, y.shape)  # Print the shapes of the feature and target arrays\n",
    "\n",
    "(3333, 2), (3333,)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=15)  # Initialize KNN classifier with 15 neighbors\n",
    "knn.fit(X, y)  # Fit the classifier to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- To fit a KNN model using scikit-learn, we import KNeighborsClassifier from sklearn-dot-neighbors. We split our data into X, a 2D array of our features, and y, a 1D array of the target values - in this case, churn status. \n",
    "- scikit-learn requires that the features are in an array where each column is a feature and each row a different observation. Similarly, the target needs to be a single column with the same number of observations as the feature data. We use the dot-values attribute to convert X and y to NumPy arrays. \n",
    "- Printing the shape of X and y, we see there are 3333 observations of two features, and 3333 observations of the target variable. \n",
    "- We then instantiate our KNeighborsClassifier, setting n_neighbors equal to 15, and assign it to the variable knn. - Then we can fit this classifier to our labeled data by applying the classifier's dot-fit method and passing two arguments: the feature values, X, and the target values, y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting on unlabeled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data points for prediction (unlabeled)\n",
    "X_new = np.array([[56.8, 17.5],\n",
    "                  [24.4, 24.1],\n",
    "                  [50.1, 10.9]])\n",
    "\n",
    "# Print the shape of the data (number of samples, number of features)\n",
    "print(X_new.shape)  # Output: (3, 2)\n",
    "\n",
    "# Assuming 'knn' is a pre-trained K-Nearest Neighbors model\n",
    "# Predict the classes for the new data points\n",
    "predictions = knn.predict(X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print('Predictions: {}'.format(predictions)) # Output: Predictions: [1 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Here we have a set of new observations, X_new. Checking the shape of X_new, we see it has three rows and two columns, that is, three observations and two features. \n",
    "- We use the classifier's dot-predict method and pass it the unseen data as a 2D NumPy array containing features in columns and observations in rows. \n",
    "- Printing the predictions returns a binary value for each observation or row in X_new. It predicts 1, which corresponds to 'churn', for the first observation, and 0, which corresponds to 'no churn', for the second and third observations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
